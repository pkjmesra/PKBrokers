#!/usr/bin/env python3
"""
Unified PKL generator for PKBrokers - handles both ticks.json and SQLite sources.

This script:
1. Downloads existing historical pkl from GitHub (~37MB)
2. Loads new data from ticks.json OR SQLite database
3. Converts to candle format
4. Merges with historical data
5. Saves as dated pkl files (~37MB+)

Usage:
    # From ticks.json (default)
    python generate_pkl_from_ticks.py [--data-dir PATH] [--verbose]
    
    # From SQLite database (for history workflow)
    python generate_pkl_from_ticks.py --from-db [--db-path PATH] [--data-dir PATH] [--verbose]
"""

import argparse
import io
import json
import os
import pickle
import sqlite3
import sys
import zipfile
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple

import pandas as pd
import requests


def log(msg: str, verbose: bool = True):
    if verbose:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")


# Global cache for token-to-symbol mapping
_token_to_symbol_cache: Dict[int, str] = {}


def get_token_to_symbol_mapping(verbose: bool = True) -> Dict[int, str]:
    """
    Get instrument token to trading symbol mapping using KiteInstruments.
    
    This uses PKBrokers' KiteInstruments class to get the authoritative
    mapping between instrument tokens and trading symbols.
    
    Returns:
        Dict mapping instrument_token (int) to tradingsymbol (str)
    """
    global _token_to_symbol_cache
    
    if _token_to_symbol_cache:
        return _token_to_symbol_cache
    
    try:
        # Add parent directory to path for imports
        script_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(os.path.dirname(script_dir))
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)
        
        from pkbrokers.kite.instruments import KiteInstruments
        from PKDevTools.classes.Environment import PKEnvironment
        
        env = PKEnvironment()
        instruments = KiteInstruments(
            api_key="kitefront",
            access_token=env.KTOKEN or "",
            local=True
        )
        
        # Get all equities with token and symbol
        equities = instruments.get_equities(
            column_names="instrument_token,tradingsymbol",
            only_nse_stocks=False
        )
        
        for eq in equities:
            token = eq.get('instrument_token')
            symbol = eq.get('tradingsymbol')
            if token and symbol:
                _token_to_symbol_cache[int(token)] = str(symbol)
        
        log(f"Loaded {len(_token_to_symbol_cache)} token-to-symbol mappings from KiteInstruments", verbose)
        
    except Exception as e:
        log(f"Warning: Could not load token mappings from KiteInstruments: {e}", verbose)
    
    return _token_to_symbol_cache


def map_tokens_to_symbols(data: Dict, verbose: bool = True) -> Dict:
    """
    Convert instrument token keys to trading symbol keys.
    
    Args:
        data: Dictionary that may have numeric keys (instrument tokens)
        verbose: Whether to log progress
        
    Returns:
        Dictionary with symbol keys instead of token keys where possible
    """
    token_to_symbol = get_token_to_symbol_mapping(verbose)
    
    if not token_to_symbol:
        log("Warning: No token-to-symbol mapping available", verbose)
        return data
    
    result = {}
    mapped_count = 0
    unmapped_count = 0
    
    for key, value in data.items():
        str_key = str(key)
        
        if str_key.isdigit():
            # This is an instrument token - try to map it
            token = int(str_key)
            symbol = token_to_symbol.get(token)
            
            if symbol:
                # Use symbol as key, but check for conflicts
                if symbol in result:
                    # Symbol already exists, keep the one with more recent data
                    existing = result[symbol]
                    new_df = value if isinstance(value, pd.DataFrame) else pd.DataFrame(value.get('data', []))
                    existing_df = existing if isinstance(existing, pd.DataFrame) else pd.DataFrame(existing.get('data', []))
                    
                    if len(new_df) > len(existing_df):
                        result[symbol] = value
                else:
                    result[symbol] = value
                mapped_count += 1
            else:
                # No mapping found - skip this entry
                unmapped_count += 1
        else:
            # Already a symbol key
            result[key] = value
    
    if mapped_count > 0:
        log(f"Mapped {mapped_count} instrument tokens to symbols", verbose)
    if unmapped_count > 0:
        log(f"Warning: Skipped {unmapped_count} unmapped instrument tokens", verbose)
    
    return result


def trim_daily_data_to_251_rows(data: Dict, verbose: bool = True) -> Dict:
    """
    Trim daily stock data to keep only the most recent 251 rows per stock.
    
    This ensures consistent file sizes and keeps approximately 1 year of trading data.
    Only applies to daily data (stock_data_*.pkl), not intraday data.
    Also filters out numeric keys (instrument tokens) which have incomplete/stale data.
    
    Args:
        data: Dictionary mapping symbol to DataFrame or dict with 'data'/'index' keys
        verbose: Whether to log progress
        
    Returns:
        Trimmed data dictionary
    """
    MAX_ROWS = 251
    trimmed_count = 0
    removed_tokens = 0
    
    for symbol in list(data.keys()):
        # Remove numeric keys (instrument tokens) - they shouldn't be in the final output
        if str(symbol).isdigit():
            del data[symbol]
            removed_tokens += 1
            continue
            
        try:
            item = data[symbol]
            
            if isinstance(item, pd.DataFrame):
                if len(item) > MAX_ROWS:
                    # Sort by index to ensure we keep the most recent
                    item = item.sort_index()
                    data[symbol] = item.tail(MAX_ROWS)
                    trimmed_count += 1
            elif isinstance(item, dict):
                # Handle dict format with 'data' and 'index' keys
                if 'data' in item and 'index' in item:
                    if len(item['data']) > MAX_ROWS:
                        # Keep last MAX_ROWS entries
                        item['data'] = item['data'][-MAX_ROWS:]
                        item['index'] = item['index'][-MAX_ROWS:]
                        trimmed_count += 1
        except Exception as e:
            log(f"âš ï¸ Error trimming {symbol}: {e}", verbose)
            continue
    
    if removed_tokens > 0:
        log(f"ðŸ”„ Removed {removed_tokens} instrument tokens (unmapped)", verbose)
    if trimmed_count > 0:
        log(f"âœ‚ï¸ Trimmed {trimmed_count} stocks to {MAX_ROWS} rows each", verbose)
    
    return data


def get_last_trading_date(verbose: bool = True):
    """Get the last trading date using PKDateUtilities."""
    try:
        from PKDevTools.classes.PKDateUtilities import PKDateUtilities
        last_trading = PKDateUtilities.tradingDate()
        if hasattr(last_trading, 'date'):
            return last_trading.date()
        return last_trading
    except Exception as e:
        log(f"âš ï¸ Could not get last trading date: {e}", verbose)
        return datetime.now().date()


def calculate_missing_trading_days(data: Dict, verbose: bool = True) -> int:
    """Calculate how many trading days are missing from the pkl data.
    
    Checks the actual date index in the data, not just the filename.
    """
    try:
        from PKDevTools.classes.PKDateUtilities import PKDateUtilities
        
        if not data:
            return 10  # Default to 10 days if no data
        
        # Find the latest date in the data by checking actual index values
        latest_date = None
        sample_symbols = ['RELIANCE', 'TCS', 'INFY', 'HDFCBANK', 'ICICIBANK', 'SBIN', 'BHARTIARTL']
        
        # Try known symbols first, then fall back to sampling
        symbols_to_check = [s for s in sample_symbols if s in data] or list(data.keys())[:50]
        
        for symbol in symbols_to_check:
            try:
                sym_data = data[symbol]
                
                # Handle dict format (with 'data', 'columns', 'index' keys)
                if isinstance(sym_data, dict) and 'index' in sym_data:
                    index_values = sym_data['index']
                    if index_values:
                        # Get the max date from the index
                        dates = pd.to_datetime(index_values, format='mixed', errors='coerce')
                        valid_dates = dates.dropna()
                        if len(valid_dates) > 0:
                            symbol_max = valid_dates.max()
                            if hasattr(symbol_max, 'date'):
                                symbol_max = symbol_max.date()
                            if latest_date is None or symbol_max > latest_date:
                                latest_date = symbol_max
                                log(f"ðŸ“Š {symbol}: latest date in data = {latest_date}", verbose)
                
                # Handle DataFrame format
                elif hasattr(sym_data, 'index') and len(sym_data) > 0:
                    symbol_max = sym_data.index.max()
                    if hasattr(symbol_max, 'date'):
                        symbol_max = symbol_max.date()
                    elif isinstance(symbol_max, str):
                        symbol_max = pd.to_datetime(symbol_max).date()
                    
                    if latest_date is None or symbol_max > latest_date:
                        latest_date = symbol_max
                        log(f"ðŸ“Š {symbol}: latest date in data = {latest_date}", verbose)
                        
            except Exception as e:
                continue
        
        if latest_date is None:
            log("âš ï¸ Could not determine latest date from pkl data", verbose)
            return 5  # Default
        
        # Get last trading date
        last_trading_date = get_last_trading_date(verbose)
        
        if latest_date >= last_trading_date:
            log(f"âœ… Data is fresh: latest={latest_date}, last_trading={last_trading_date}", verbose)
            return 0
        
        # Calculate missing trading days
        try:
            missing_days = PKDateUtilities.trading_days_between(latest_date, last_trading_date)
            log(f"ðŸ“… Data is stale: latest={latest_date}, last_trading={last_trading_date}, missing={missing_days} trading days", verbose)
            return missing_days
        except:
            # Fallback: simple calendar day difference
            diff = (last_trading_date - latest_date).days
            missing_days = max(1, diff // 2)  # Rough estimate: ~half are trading days
            log(f"ðŸ“… Data is stale: latest={latest_date}, missing ~{missing_days} trading days (estimated)", verbose)
            return missing_days
            
    except Exception as e:
        log(f"âš ï¸ Error calculating missing days: {e}", verbose)
        return 5  # Default


def download_historical_pkl(verbose: bool = True) -> Tuple[Optional[Dict], int]:
    """Download the most recent historical pkl from GitHub.
    
    Returns:
        Tuple of (data_dict, missing_trading_days)
    """
    
    # Try multiple locations and date formats
    base_urls = [
        "https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/actions-data-download/",
        "https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/results/Data/",
    ]
    
    # Try last 10 days
    today = datetime.now()
    # Look back up to 30 days to find a valid pkl file with quality data
    for days_back in range(30):
        check_date = today - timedelta(days=days_back)
        
        # Try different date formats
        date_formats = [
            check_date.strftime('%d%m%Y'),  # 29122025
            check_date.strftime('%d%m%y'),   # 291225
            f"{check_date.day}{check_date.strftime('%m%y')}",  # 291225 without leading zero
        ]
        
        for base_url in base_urls:
            for date_str in date_formats:
                url = f"{base_url}stock_data_{date_str}.pkl"
                try:
                    log(f"Trying: {url}", verbose)
                    response = requests.get(url, timeout=30)
                    if response.status_code == 200 and len(response.content) > 1000000:  # > 1MB
                        data = pickle.loads(response.content)
                        if isinstance(data, dict) and len(data) > 100:
                            # Validate data quality - check rows per stock
                            sample_symbols = ['RELIANCE', 'TCS', 'INFY', 'HDFCBANK', 'ICICIBANK']
                            min_rows = 100  # Expect at least 100 rows for valid historical data
                            
                            rows_ok = False
                            for sym in sample_symbols:
                                if sym in data:
                                    sym_data = data[sym]
                                    if isinstance(sym_data, dict) and 'data' in sym_data:
                                        row_count = len(sym_data.get('data', []))
                                    elif hasattr(sym_data, '__len__'):
                                        row_count = len(sym_data)
                                    else:
                                        row_count = 0
                                    
                                    if row_count >= min_rows:
                                        rows_ok = True
                                        log(f"âœ… Data quality check passed: {sym} has {row_count} rows", verbose)
                                        break
                                    else:
                                        log(f"âš ï¸ Data quality issue: {sym} has only {row_count} rows (need {min_rows}+)", verbose)
                            
                            if not rows_ok:
                                log(f"âš ï¸ Skipping {url} - insufficient historical data", verbose)
                                continue
                            
                            # Also validate that the actual data date is close to the filename date
                            # If data is too old (> 20 trading days behind), skip and look for better file
                            actual_missing = calculate_missing_trading_days(data, verbose)
                            if actual_missing > 20:
                                log(f"âš ï¸ Skipping {url} - data too stale ({actual_missing} trading days behind)", verbose)
                                continue
                            
                            log(f"âœ… Downloaded historical pkl: {len(data)} instruments, {len(response.content)/1024/1024:.1f} MB", verbose)
                            
                            return data, actual_missing
                except Exception as e:
                    continue
    
    log("âŒ Could not download historical pkl from GitHub", verbose)
    return None, 0


def download_ticks_json(verbose: bool = True) -> Optional[Dict]:
    """Download ticks.json from GitHub."""
    
    urls = [
        # Primary location - actions-data-download subdirectory
        "https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/actions-data-download/ticks.json.zip",
        "https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/actions-data-download/ticks.json",
        # Fallback locations
        "https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/results/Data/ticks.json.zip",
        "https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/results/Data/ticks.json",
        "https://raw.githubusercontent.com/pkjmesra/PKBrokers/main/pkbrokers/kite/examples/results/Data/ticks.json",
    ]
    
    for url in urls:
        try:
            log(f"Trying ticks: {url}", verbose)
            response = requests.get(url, timeout=30)
            if response.status_code != 200:
                continue
            
            if url.endswith('.zip'):
                with zipfile.ZipFile(io.BytesIO(response.content)) as zf:
                    if 'ticks.json' in zf.namelist():
                        with zf.open('ticks.json') as f:
                            data = json.load(f)
                    else:
                        continue
            else:
                data = response.json()
            
            if isinstance(data, dict) and len(data) > 100:
                log(f"âœ… Downloaded ticks.json: {len(data)} instruments", verbose)
                return data
        except Exception as e:
            continue
    
    log("âŒ Could not download ticks.json from GitHub", verbose)
    return None


def load_local_ticks_json(data_dir: str, verbose: bool = True, min_instruments: int = 10) -> Optional[Dict]:
    """Load ticks.json from local path.
    
    Args:
        data_dir: Directory to search for ticks.json
        verbose: Whether to log progress
        min_instruments: Minimum number of instruments required (default 10)
    
    Returns:
        Dict of ticks data or None if not found/too small
    """
    
    paths = [
        os.path.join(data_dir, "ticks.json"),
        os.path.join(data_dir, "results", "Data", "ticks.json"),
        "ticks.json",
        "results/Data/ticks.json",
    ]
    
    for path in paths:
        if os.path.exists(path):
            try:
                file_size = os.path.getsize(path)
                if file_size < 100:  # Skip tiny files (empty or nearly empty)
                    log(f"âš ï¸ Skipping tiny ticks.json: {path} ({file_size} bytes)", verbose)
                    continue
                    
                with open(path, 'r') as f:
                    data = json.load(f)
                if isinstance(data, dict) and len(data) >= min_instruments:
                    log(f"âœ… Loaded local ticks.json: {path} ({len(data)} instruments)", verbose)
                    return data
                else:
                    log(f"âš ï¸ ticks.json has too few instruments: {len(data) if isinstance(data, dict) else 0}", verbose)
            except Exception as e:
                log(f"âš ï¸ Error loading {path}: {e}", verbose)
                continue
    
    return None


def find_sqlite_database(verbose: bool = True) -> Optional[str]:
    """Find SQLite history database in common locations."""
    
    search_paths = [
        os.path.expanduser('~/.PKDevTools_userdata'),
        os.path.expanduser('~/.pkbrokers'),
        '.',
        'results/Data',
    ]
    
    db_names = ['instrument_history.db', 'kite_history.db', 'history.db']
    
    for search_dir in search_paths:
        if not os.path.exists(search_dir):
            continue
        for db_name in db_names:
            db_path = os.path.join(search_dir, db_name)
            if os.path.exists(db_path):
                log(f"âœ… Found database: {db_path}", verbose)
                return db_path
        # Also search for any .db files with 'history' in name
        try:
            for f in os.listdir(search_dir):
                if f.endswith('.db') and 'history' in f.lower():
                    db_path = os.path.join(search_dir, f)
                    log(f"âœ… Found database: {db_path}", verbose)
                    return db_path
        except:
            continue
    
    log("âŒ No SQLite database found", verbose)
    return None


def load_from_sqlite(db_path: str, verbose: bool = True) -> Dict:
    """Load daily candles from SQLite database."""
    
    candles = {}
    
    if not db_path or not os.path.exists(db_path):
        log(f"Database not found: {db_path}", verbose)
        return candles
    
    try:
        conn = sqlite3.connect(db_path)
        
        # Check available tables
        tables_df = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table'", conn)
        table_list = tables_df['name'].tolist()
        log(f"Tables in database: {table_list}", verbose)
        
        df = pd.DataFrame()
        has_instruments_table = 'instruments' in table_list
        
        # Try to JOIN with instruments table to get tradingsymbols directly
        if has_instruments_table and 'instrument_history' in table_list:
            try:
                query = """
                SELECT ih.instrument_token, ih.timestamp, ih.open, ih.high, ih.low, ih.close, ih.volume,
                       i.tradingsymbol
                FROM instrument_history ih
                JOIN instruments i ON ih.instrument_token = i.instrument_token
                WHERE ih.interval = 'day' OR ih.interval IS NULL
                ORDER BY ih.instrument_token, ih.timestamp
                """
                df = pd.read_sql_query(query, conn)
                if len(df) > 0:
                    log(f"Loaded {len(df)} rows with JOINed tradingsymbols from instrument_history + instruments", verbose)
            except Exception as e:
                log(f"JOIN query failed: {e}, falling back to separate queries", verbose)
                df = pd.DataFrame()
        
        # Fall back to loading instrument_history without JOIN
        if len(df) == 0:
            for table_name in ['instrument_history', 'history', 'candles', 'daily_candles']:
                try:
                    query = f"""
                    SELECT instrument_token, timestamp, open, high, low, close, volume
                    FROM {table_name}
                    WHERE interval = 'day' OR interval IS NULL
                    ORDER BY instrument_token, timestamp
                    """
                    df = pd.read_sql_query(query, conn)
                    if len(df) > 0:
                        log(f"Loaded {len(df)} rows from {table_name}", verbose)
                        break
                except Exception as e:
                    continue
        
        if len(df) == 0:
            log("No data found in database tables", verbose)
            conn.close()
            return candles
        
        # Check if we have tradingsymbol column from JOIN
        has_tradingsymbol = 'tradingsymbol' in df.columns
        
        # If no tradingsymbol from JOIN, try to load symbol mapping from instruments table
        token_to_symbol = {}
        if not has_tradingsymbol and has_instruments_table:
            try:
                instruments_df = pd.read_sql_query(
                    "SELECT instrument_token, tradingsymbol FROM instruments",
                    conn
                )
                for _, row in instruments_df.iterrows():
                    token_to_symbol[row['instrument_token']] = row['tradingsymbol']
                log(f"Loaded {len(token_to_symbol)} symbol mappings from instruments table", verbose)
            except Exception as e:
                log(f"Could not load from instruments table: {e}", verbose)
        
        # If still no symbols, try loading from separate instruments.db file
        if not has_tradingsymbol and len(token_to_symbol) == 0:
            # instruments.db is typically in the same directory as instrument_history.db
            db_dir = os.path.dirname(db_path)
            instruments_db_paths = [
                os.path.join(db_dir, "instruments.db"),
                os.path.join(os.path.dirname(db_dir), "instruments.db"),
                os.path.join(os.getcwd(), "instruments.db"),
            ]
            # Also check common user data directories
            try:
                from PKDevTools.classes import Archiver
                instruments_db_paths.append(os.path.join(Archiver.get_user_data_dir(), "instruments.db"))
            except:
                pass
            
            for inst_db_path in instruments_db_paths:
                if os.path.exists(inst_db_path):
                    try:
                        inst_conn = sqlite3.connect(inst_db_path)
                        instruments_df = pd.read_sql_query(
                            "SELECT instrument_token, tradingsymbol FROM instruments",
                            inst_conn
                        )
                        for _, row in instruments_df.iterrows():
                            token_to_symbol[row['instrument_token']] = row['tradingsymbol']
                        inst_conn.close()
                        log(f"Loaded {len(token_to_symbol)} symbol mappings from {inst_db_path}", verbose)
                        break
                    except Exception as e:
                        log(f"Could not load from {inst_db_path}: {e}", verbose)
        
        # If still no symbols, try PKBrokers KiteInstruments
        if not has_tradingsymbol and len(token_to_symbol) == 0:
            try:
                sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
                from pkbrokers.kite.instruments import KiteInstruments
                from PKDevTools.classes.Environment import PKEnvironment
                
                env = PKEnvironment()
                instruments = KiteInstruments(
                    api_key="kitefront",
                    access_token=env.KTOKEN or "",
                    local=True
                )
                for inst in instruments.get_or_fetch_instrument_tokens(all_columns=True):
                    if isinstance(inst, dict):
                        token_to_symbol[inst.get('instrument_token')] = inst.get('tradingsymbol', str(inst.get('instrument_token')))
                log(f"Loaded {len(token_to_symbol)} symbol mappings from KiteInstruments", verbose)
            except Exception as e:
                log(f"Could not load symbol mapping from KiteInstruments: {e}", verbose)
                log("Using instrument tokens as symbol names", verbose)
        
        # Convert to pkl format
        for token, group in df.groupby('instrument_token'):
            # Get symbol from tradingsymbol column, or from mapping, or use token
            if has_tradingsymbol:
                symbol = group['tradingsymbol'].iloc[0]
                if pd.isna(symbol) or not symbol:
                    symbol = token_to_symbol.get(token, str(token))
            else:
                symbol = token_to_symbol.get(token, str(token))
            
            cols_to_use = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
            group_df = group[cols_to_use].copy()
            # Parse timestamps and ensure tz-naive for consistent merging
            group_df['timestamp'] = pd.to_datetime(group_df['timestamp'], format='mixed', utc=True)
            group_df.set_index('timestamp', inplace=True)
            # Convert to tz-naive
            if hasattr(group_df.index, 'tz') and group_df.index.tz is not None:
                group_df.index = group_df.index.tz_localize(None)
            group_df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
            candles[symbol] = group_df
        
        conn.close()
        log(f"Converted {len(candles)} instruments from database", verbose)
        
    except Exception as e:
        log(f"Error loading from database: {e}", verbose)
    
    return candles


def convert_ticks_to_candles(ticks_data: Dict, verbose: bool = True) -> Dict:
    """Convert ticks.json format to DataFrame candle format."""
    
    candles = {}
    
    for token_str, tick_info in ticks_data.items():
        try:
            symbol = tick_info.get('trading_symbol', str(token_str))
            ohlcv = tick_info.get('ohlcv', {})
            
            if not ohlcv:
                continue
            
            close = float(ohlcv.get('close', 0))
            if close <= 0:
                continue
            
            # Parse timestamp and ensure tz-naive
            timestamp_str = ohlcv.get('timestamp', '')
            if timestamp_str:
                try:
                    dt = pd.to_datetime(timestamp_str, utc=True)
                    # Convert to tz-naive
                    if hasattr(dt, 'tz') and dt.tz is not None:
                        dt = dt.tz_localize(None)
                except:
                    dt = datetime.now()
            else:
                dt = datetime.now()
            
            # Create DataFrame with tz-naive index
            df = pd.DataFrame([{
                'Open': float(ohlcv.get('open', close)),
                'High': float(ohlcv.get('high', close)),
                'Low': float(ohlcv.get('low', close)),
                'Close': close,
                'Volume': int(ohlcv.get('volume', 0)),
            }], index=[dt])
            
            # Ensure index is tz-naive
            if hasattr(df.index, 'tz') and df.index.tz is not None:
                df.index = df.index.tz_localize(None)
            
            candles[symbol] = df
            
        except Exception as e:
            continue
    
    log(f"Converted {len(candles)} instruments from ticks to candles", verbose)
    return candles


def _normalize_index_tz(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize DataFrame index to timezone-naive for consistent merging."""
    if df is None or not hasattr(df, 'index'):
        return df
    
    try:
        if hasattr(df.index, 'tz') and df.index.tz is not None:
            # Convert tz-aware to tz-naive (UTC first, then remove tz)
            df = df.copy()
            df.index = df.index.tz_convert('UTC').tz_localize(None)
    except Exception:
        pass
    
    return df


def merge_candles(historical: Dict, today: Dict, verbose: bool = True) -> Dict:
    """Merge today's candles with historical data."""
    
    if not historical:
        log("No historical data to merge with", verbose)
        return today
    
    # First, map any instrument tokens to symbols in both datasets
    log("Mapping instrument tokens to symbols...", verbose)
    historical = map_tokens_to_symbols(historical, verbose)
    today = map_tokens_to_symbols(today, verbose)
    
    merged = {}
    today_date = datetime.now().date()
    
    # Start with all historical data (skip any remaining numeric keys)
    for symbol, hist_df in historical.items():
        # Skip numeric keys (instrument tokens) that couldn't be mapped
        if str(symbol).isdigit():
            continue
            
        if isinstance(hist_df, dict):
            # Convert dict format to DataFrame
            if 'data' in hist_df and 'columns' in hist_df:
                hist_df = pd.DataFrame(hist_df['data'], columns=hist_df['columns'])
                if 'index' in historical[symbol]:
                    # Use format='mixed' to handle various ISO8601 formats with timezone/microseconds
                    hist_df.index = pd.to_datetime(historical[symbol]['index'], format='mixed', utc=True)
                    # Convert to timezone-naive for consistency
                    hist_df.index = hist_df.index.tz_localize(None)
        
        if hasattr(hist_df, 'index'):
            # Ensure timezone-naive
            hist_df = _normalize_index_tz(hist_df)
            merged[symbol] = hist_df.copy()
    
    # Add/update with today's data
    updated_count = 0
    new_count = 0
    skipped_tokens = 0
    
    for symbol, today_df in today.items():
        # Skip numeric keys (instrument tokens) that couldn't be mapped
        if str(symbol).isdigit():
            skipped_tokens += 1
            continue
            
        # Ensure today's data is also timezone-naive
        today_df = _normalize_index_tz(today_df)
        
        if symbol in merged:
            # Append today's data, removing duplicates
            existing = merged[symbol]
            # Ensure both are tz-naive before concatenation
            existing = _normalize_index_tz(existing)
            combined = pd.concat([existing, today_df])
            combined = combined[~combined.index.duplicated(keep='last')]
            combined = combined.sort_index()
            merged[symbol] = combined
            updated_count += 1
        else:
            merged[symbol] = today_df
            new_count += 1
    
    if skipped_tokens > 0:
        log(f"Warning: Skipped {skipped_tokens} unmappable instrument tokens", verbose)
    log(f"Merged: {updated_count} updated, {new_count} new, {len(merged)} total instruments", verbose)
    return merged


def trigger_history_download(missing_days: int, verbose: bool = True) -> bool:
    """Trigger the history download workflow via GitHub API.
    
    Args:
        missing_days: Number of trading days to fetch
        verbose: Whether to log progress
        
    Returns:
        True if workflow was triggered successfully
    """
    try:
        ci_pat = os.environ.get('CI_PAT') or os.environ.get('GITHUB_TOKEN')
        if not ci_pat:
            log("âš ï¸ No CI_PAT or GITHUB_TOKEN available to trigger workflow", verbose)
            return False
        
        url = "https://api.github.com/repos/pkjmesra/PKBrokers/actions/workflows/w1-workflow-history-data-child.yml/dispatches"
        
        headers = {
            "Authorization": f"token {ci_pat}",
            "Accept": "application/vnd.github.v3+json"
        }
        
        payload = {
            "ref": "main",
            "inputs": {
                "period": "day",
                "pastoffset": str(missing_days),
                "logLevel": "20"
            }
        }
        
        log(f"ðŸš€ Triggering history download workflow with pastoffset={missing_days}...", verbose)
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        
        if response.status_code == 204:
            log("âœ… History download workflow triggered successfully", verbose)
            return True
        else:
            log(f"âš ï¸ Failed to trigger workflow: {response.status_code} - {response.text}", verbose)
            return False
            
    except Exception as e:
        log(f"âš ï¸ Error triggering history workflow: {e}", verbose)
        return False


def save_pkl_files(data: Dict, data_dir: str, verbose: bool = True) -> Tuple[str, str]:
    """Save pkl files with both generic and dated names.
    
    Daily data is trimmed to 251 rows per stock before saving.
    """
    
    os.makedirs(data_dir, exist_ok=True)
    
    today = datetime.now().strftime('%d%m%Y')
    
    # Trim daily data to 251 rows per stock before saving
    data = trim_daily_data_to_251_rows(data, verbose)
    
    # Save daily pkl
    daily_path = os.path.join(data_dir, f"stock_data_{today}.pkl")
    with open(daily_path, 'wb') as f:
        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    daily_size = os.path.getsize(daily_path) / (1024 * 1024)
    log(f"âœ… Saved daily pkl: {daily_path} ({daily_size:.2f} MB, {len(data)} instruments)", verbose)
    
    # Also save as generic name
    generic_path = os.path.join(data_dir, "daily_candles.pkl")
    with open(generic_path, 'wb') as f:
        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    return daily_path, generic_path


def save_intraday_pkl(ticks_candles: Dict, data_dir: str, verbose: bool = True) -> str:
    """Save intraday pkl from today's ticks."""
    
    os.makedirs(data_dir, exist_ok=True)
    
    today = datetime.now().strftime('%d%m%Y')
    
    intraday_path = os.path.join(data_dir, f"intraday_stock_data_{today}.pkl")
    with open(intraday_path, 'wb') as f:
        pickle.dump(ticks_candles, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    size = os.path.getsize(intraday_path) / (1024 * 1024)
    log(f"âœ… Saved intraday pkl: {intraday_path} ({size:.2f} MB, {len(ticks_candles)} instruments)", verbose)
    
    # Also save as generic name
    generic_path = os.path.join(data_dir, "intraday_1m_candles.pkl")
    with open(generic_path, 'wb') as f:
        pickle.dump(ticks_candles, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    return intraday_path


def main():
    parser = argparse.ArgumentParser(description="Generate pkl files from ticks.json or SQLite database")
    parser.add_argument("--data-dir", default="results/Data", help="Output directory for pkl files")
    parser.add_argument("--from-db", action="store_true", help="Load data from SQLite database instead of ticks.json")
    parser.add_argument("--db-path", default=None, help="Path to SQLite database (auto-detected if not specified)")
    parser.add_argument("--trigger-history", action="store_true", help="Trigger history download workflow if data is stale")
    parser.add_argument("--verbose", "-v", action="store_true", default=True, help="Verbose output")
    args = parser.parse_args()
    
    verbose = args.verbose
    data_dir = args.data_dir
    
    log("=" * 60, verbose)
    log("PKL Generator: Unified pkl file generation", verbose)
    log(f"Mode: {'SQLite Database' if args.from_db else 'Ticks JSON'}", verbose)
    log("=" * 60, verbose)
    
    new_candles = {}
    
    # Step 1: Always download historical pkl first (this is our base)
    log("\n[Step 1] Downloading historical pkl from GitHub...", verbose)
    historical_data, missing_trading_days = download_historical_pkl(verbose)
    
    if historical_data:
        log(f"Historical data: {len(historical_data)} instruments", verbose)
        if missing_trading_days > 0:
            log(f"âš ï¸ Historical data is missing {missing_trading_days} trading days", verbose)
            
            # Trigger history download workflow if requested
            if args.trigger_history:
                trigger_history_download(missing_trading_days, verbose)
        else:
            # Data is already fresh - no need to fetch again
            log("âœ… Historical data is already up-to-date, skipping new data fetch", verbose)
            log("\n[Step 4] Saving pkl files (using existing fresh data)...", verbose)
            save_pkl_files(historical_data, data_dir, verbose)
            log("=" * 60, verbose)
            log("âœ… SUCCESS: PKL files saved (data was already fresh)", verbose)
            log("=" * 60, verbose)
            return
    else:
        log("âš ï¸ No historical pkl found on GitHub", verbose)
        missing_trading_days = 0
        
        # If no historical data and trigger is enabled, fetch last 10 days
        if args.trigger_history:
            trigger_history_download(10, verbose)
    
    # Step 2: Load new data based on mode
    log("\n[Step 2] Loading new data...", verbose)
    
    if args.from_db:
        # Load from SQLite database
        db_path = args.db_path if args.db_path else find_sqlite_database(verbose)
        if db_path:
            new_candles = load_from_sqlite(db_path, verbose)
        
        if not new_candles:
            log("âš ï¸ No data from database, trying ticks.json as fallback...", verbose)
            ticks_data = load_local_ticks_json(data_dir, verbose)
            if ticks_data:
                new_candles = convert_ticks_to_candles(ticks_data, verbose)
    else:
        # Load from ticks.json
        ticks_data = load_local_ticks_json(data_dir, verbose)
        if not ticks_data:
            ticks_data = download_ticks_json(verbose)
        
        if ticks_data:
            new_candles = convert_ticks_to_candles(ticks_data, verbose)
            # Save intraday pkl (just today's ticks) - only if we have meaningful data
            if new_candles and len(new_candles) >= 10:
                save_intraday_pkl(new_candles, data_dir, verbose)
            else:
                log("âš ï¸ Not enough new data for intraday pkl", verbose)
    
    # Step 3: Determine what to save
    log("\n[Step 3] Preparing final data...", verbose)
    
    if not historical_data and not new_candles:
        log("âŒ FAILED: No data available (neither historical nor new)", verbose)
        sys.exit(1)
    
    # Merge or use what we have
    if new_candles and len(new_candles) >= 10:
        merged_data = merge_candles(historical_data, new_candles, verbose)
    elif historical_data:
        merged_data = historical_data
        log("Using historical data only (no new data to merge)", verbose)
    else:
        merged_data = new_candles
        log("Using new data only (no historical data found)", verbose)
    
    # Step 4: Save merged daily pkl
    log("\n[Step 4] Saving pkl files...", verbose)
    save_pkl_files(merged_data, data_dir, verbose)
    
    log("=" * 60, verbose)
    log("âœ… SUCCESS: PKL files generated", verbose)
    log("=" * 60, verbose)


if __name__ == "__main__":
    main()

















