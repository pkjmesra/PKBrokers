# """
#     The MIT License (MIT)

#     Copyright (c) 2023 pkjmesra

#     Permission is hereby granted, free of charge, to any person obtaining a copy
#     of this software and associated documentation files (the "Software"), to deal
#     in the Software without restriction, including without limitation the rights
#     to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
#     copies of the Software, and to permit persons to whom the Software is
#     furnished to do so, subject to the following conditions:

#     The above copyright notice and this permission notice shall be included in all
#     copies or substantial portions of the Software.

#     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
#     IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
#     FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
#     AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
#     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
#     OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
#     SOFTWARE.

# """

name: 1. History Data Child
on:
  workflow_dispatch:  # Allow manual triggering
    inputs:
      logLevel:
        description: 'Log level for PKDevTools (default 20)'
        required: false
        default: 20
        type: number
      period:
        description: 'Specific period to run (default: day)'
        required: false
        default: 'day'
        type: string
      kiteToken:
        description: 'Kite API token'
        required: false
        type: string
      pastoffset:
        description: 'Past number of days for which data has to be fetched'
        required: false
        default: 0
        type: number

jobs:
  historical-data:
    if: (github.event_name == 'workflow_dispatch' || github.event_name == 'schedule')
    runs-on: ubuntu-latest
    timeout-minutes: 358
    env:
      PKDevTools_Default_Log_Level: ${{ github.event.inputs.logLevel || 20 }}
      TEL_SESSION_DATA: ${{ secrets.TEL_SESSION_DATA }}
    steps:

    - name: Mask sensitive outputs
      run: |
        # Mask the token output from inputs
        echo "::add-mask::${{ inputs.kiteToken || secrets.KTOKEN }}"

    - name: Checkout PKBrokers code
      uses: actions/checkout@v4
      with:
        repository: pkjmesra/PKBrokers
        ref: main

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: pip install -e . kiteconnect

    - name: Create .env.dev file with secrets
      env:
        CHAT_ID: ${{ secrets.CHAT_ID }}
        TOKEN: ${{ secrets.TOKEN }}
        chat_idADMIN: ${{ secrets.chat_idADMIN }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        CI_PAT: ${{ secrets.CI_PAT }}
        TURSO_TOKEN: ${{ secrets.TURSO_TOKEN }}
        TDU: ${{ secrets.TDU }}
        TAT: ${{ secrets.TAT }}
        MCU: ${{ secrets.MCU }}
        MCAP: ${{ secrets.MCAP }}
        MCL: ${{ secrets.MCL }}
        MS: ${{ secrets.MS }}
        PKG: ${{ secrets.PKG }}
        REPO_URL: ${{ secrets.REPO_URL }}
        KTOKEN: ${{ inputs.kiteToken || secrets.KTOKEN }}
        KUSER: ${{ secrets.KUSER }}
        KPWD: ${{ secrets.KPWD }}
        KTOTP: ${{ secrets.KTOTP }}
        DB_TYPE: ${{ secrets.DB_TYPE }}
        TBTOKEN: ${{ secrets.TBTOKEN }}
        Tel_API_ID: ${{ secrets.Tel_API_ID }}
        Tel_API_Hash: ${{ secrets.Tel_API_Hash }}
        Tel_Phone_Number: ${{ secrets.Tel_Phone_Number }}
        TEL_SESSION_DATA: ${{ secrets.TEL_SESSION_DATA }}
        PERIOD: ${{ inputs.period }}
      run: |
        cat > .env.dev << EOF
        CHAT_ID=$CHAT_ID
        TOKEN=$TOKEN
        chat_idADMIN=$chat_idADMIN
        GITHUB_TOKEN=$GITHUB_TOKEN
        CI_PAT=$CI_PAT
        TURSO_TOKEN=$TURSO_TOKEN
        TDU=$TDU
        TAT=$TAT
        MCU=$MCU
        MCAP=$MCAP
        MCL=$MCL
        MS=$MS
        PKG=$PKG
        REPO_URL=$REPO_URL
        KTOKEN=$KTOKEN
        KUSER=$KUSER
        KPWD=$KPWD
        KTOTP=$KTOTP
        DB_TYPE=$DB_TYPE
        TBTOKEN=$TBTOKEN
        PERIOD=$PERIOD
        Tel_API_ID=$Tel_API_ID
        Tel_API_Hash=$Tel_API_Hash
        Tel_Phone_Number=$Tel_Phone_Number
        TEL_SESSION_DATA=$TEL_SESSION_DATA
        PKDevTools_Default_Log_Level=${{ env.PKDevTools_Default_Log_Level }}
        EOF
        
        echo "Created .env.dev file for period: day"

    - name: Create session file
      run: |
        echo ${{ env.TEL_SESSION_DATA }} | base64 -d > user_session.session
        chmod 600 user_session.session
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        echo ${{ env.TEL_SESSION_DATA }} | base64 -d > user_session.session
        chmod 600 user_session.session

    - name: Run historical data for ${{ inputs.period }}
      run: |
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        python pkkite.py --history=${{ inputs.period }} --pastoffset=${{ inputs.pastoffset || 0}} --verbose

    - name: Export database to pkl files with historical merge
      if: inputs.period == 'day'
      run: |
        cd pkbrokers/kite/examples/
        mkdir -p results/Data
        
        python3 << 'EXPORT_SCRIPT'
        import os
        import pickle
        import sqlite3
        import io
        import requests
        from datetime import datetime, timedelta
        import pandas as pd
        
        print("=" * 60)
        print("PKL Export: Database to PKL with Historical Merge")
        print("=" * 60)
        
        # Step 1: Download existing historical pkl from GitHub
        print("\n[Step 1] Downloading historical pkl from GitHub...")
        historical_data = {}
        
        base_urls = [
            "https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/actions-data-download/",
            "https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/results/Data/",
        ]
        
        today = datetime.now()
        for days_back in range(10):
            check_date = today - timedelta(days=days_back)
            date_formats = [
                check_date.strftime('%d%m%Y'),
                check_date.strftime('%d%m%y'),
            ]
            
            for base_url in base_urls:
                for date_str in date_formats:
                    url = f"{base_url}stock_data_{date_str}.pkl"
                    try:
                        response = requests.get(url, timeout=60)
                        if response.status_code == 200 and len(response.content) > 1000000:
                            historical_data = pickle.loads(response.content)
                            if isinstance(historical_data, dict) and len(historical_data) > 100:
                                print(f"✅ Downloaded: {url}")
                                print(f"   {len(historical_data)} instruments, {len(response.content)/1024/1024:.1f} MB")
                                break
                    except Exception as e:
                        continue
                if historical_data:
                    break
            if historical_data:
                break
        
        if not historical_data:
            print("⚠️ Could not download historical pkl from GitHub")
        
        # Step 2: Find and load the history database
        print("\n[Step 2] Finding history database...")
        db_path = None
        
        # Search paths including PKDevTools user data dir
        search_paths = [
            os.path.expanduser('~/.PKDevTools_userdata'),
            '.',
            'results/Data',
        ]
        
        for search_dir in search_paths:
            if not os.path.exists(search_dir):
                continue
            print(f"  Searching: {search_dir}")
            for f in os.listdir(search_dir):
                if f.endswith('.db') and ('history' in f.lower() or 'instrument' in f.lower()):
                    candidate = os.path.join(search_dir, f)
                    print(f"  Found: {candidate}")
                    db_path = candidate
                    break
            if db_path:
                break
        
        new_data = {}
        if db_path and os.path.exists(db_path):
            print(f"✅ Using database: {db_path}")
            conn = sqlite3.connect(db_path)
            
            tables = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table'", conn)
            print(f"   Tables: {tables['name'].tolist()}")
            
            # Query daily candles
            for table_name in ['instrument_history', 'history', 'candles']:
                try:
                    query = f"""
                    SELECT instrument_token, timestamp, open, high, low, close, volume
                    FROM {table_name}
                    WHERE interval = 'day' OR interval IS NULL
                    ORDER BY instrument_token, timestamp
                    """
                    df = pd.read_sql_query(query, conn)
                    if len(df) > 0:
                        print(f"   Loaded {len(df)} rows from {table_name}")
                        break
                except:
                    continue
            else:
                df = pd.DataFrame()
            
            if len(df) > 0:
                # Get symbol mapping
                token_to_symbol = {}
                try:
                    from pkbrokers.kite.instruments import KiteInstruments
                    instruments = KiteInstruments()
                    for inst in instruments.get_or_fetch_instrument_tokens(all_columns=True):
                        if isinstance(inst, dict):
                            token_to_symbol[inst.get('instrument_token')] = inst.get('tradingsymbol', str(inst.get('instrument_token')))
                    print(f"   Loaded {len(token_to_symbol)} symbol mappings")
                except Exception as e:
                    print(f"   Symbol mapping error: {e}")
                
                # Convert to pkl format
                for token, group in df.groupby('instrument_token'):
                    symbol = token_to_symbol.get(token, str(token))
                    group_df = group[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()
                    group_df['timestamp'] = pd.to_datetime(group_df['timestamp'])
                    group_df.set_index('timestamp', inplace=True)
                    group_df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
                    new_data[symbol] = group_df
                
                print(f"   Converted {len(new_data)} instruments from database")
            conn.close()
        else:
            print("⚠️ No history database found")
            print("   Files in ~/.PKDevTools_userdata:")
            user_dir = os.path.expanduser('~/.PKDevTools_userdata')
            if os.path.exists(user_dir):
                for f in os.listdir(user_dir):
                    print(f"     {f}")
        
        # Step 3: Merge new data with historical
        print("\n[Step 3] Merging data...")
        merged_data = historical_data.copy() if historical_data else {}
        
        updated = 0
        added = 0
        for symbol, new_df in new_data.items():
            if symbol in merged_data:
                existing = merged_data[symbol]
                if hasattr(existing, 'index'):
                    combined = pd.concat([existing, new_df])
                    combined = combined[~combined.index.duplicated(keep='last')]
                    combined = combined.sort_index()
                    merged_data[symbol] = combined
                    updated += 1
                else:
                    merged_data[symbol] = new_df
                    added += 1
            else:
                merged_data[symbol] = new_df
                added += 1
        
        print(f"   Updated: {updated}, Added: {added}, Total: {len(merged_data)}")
        
        # Step 4: Save pkl files
        print("\n[Step 4] Saving pkl files...")
        if merged_data:
            today_str = datetime.now().strftime('%d%m%Y')
            
            pkl_path = f"results/Data/stock_data_{today_str}.pkl"
            with open(pkl_path, 'wb') as f:
                pickle.dump(merged_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            size_mb = os.path.getsize(pkl_path) / (1024 * 1024)
            print(f"✅ Saved: {pkl_path} ({size_mb:.2f} MB, {len(merged_data)} instruments)")
            
            with open("results/Data/daily_candles.pkl", 'wb') as f:
                pickle.dump(merged_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            print(f"✅ Saved: results/Data/daily_candles.pkl")
        else:
            print("❌ No data to save!")
        
        print("\n" + "=" * 60)
        EXPORT_SCRIPT

    - name: Commit updated pkl file to PKScreener actions-data-download
      if: inputs.period == 'day'
      env:
        CI_PAT: ${{ secrets.CI_PAT }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Clone PKScreener actions-data-download branch
        git clone --single-branch --branch actions-data-download https://x-access-token:$CI_PAT@github.com/pkjmesra/PKScreener.git pkscreener-data
        cd pkscreener-data
        
        # Get today's date in DDMMYYYY format
        TODAY=$(date +%d%m%Y)
        
        echo "Looking for pkl files to copy..."
        echo "Today's suffix: $TODAY"
        
        # List available files
        echo "Files in ../pkbrokers/kite/examples/results/Data/:"
        ls -la ../pkbrokers/kite/examples/results/Data/ 2>/dev/null || echo "Directory not found"
        
        # Copy pkl files - try both date-suffixed and generic names
        FILES_COPIED=0
        
        # Daily pkl files
        for PKL_FILE in ../pkbrokers/kite/examples/results/Data/stock_data_*.pkl ../pkbrokers/kite/examples/results/Data/daily_candles.pkl; do
          if [ -f "$PKL_FILE" ]; then
            PKL_SIZE=$(stat -c%s "$PKL_FILE" 2>/dev/null || stat -f%z "$PKL_FILE")
            if [ "$PKL_SIZE" -gt 1000000 ]; then
              echo "✅ Found: $PKL_FILE ($PKL_SIZE bytes)"
              cp "$PKL_FILE" actions-data-download/stock_data_${TODAY}.pkl
              cp "$PKL_FILE" actions-data-download/daily_candles.pkl
              FILES_COPIED=$((FILES_COPIED + 1))
              break
            else
              echo "⚠️ File too small: $PKL_FILE ($PKL_SIZE bytes)"
            fi
          fi
        done
        
        # Intraday pkl files
        for PKL_FILE in ../pkbrokers/kite/examples/results/Data/intraday_stock_data_*.pkl ../pkbrokers/kite/examples/results/Data/intraday_1m_candles.pkl; do
          if [ -f "$PKL_FILE" ]; then
            PKL_SIZE=$(stat -c%s "$PKL_FILE" 2>/dev/null || stat -f%z "$PKL_FILE")
            if [ "$PKL_SIZE" -gt 1000 ]; then
              echo "✅ Found: $PKL_FILE ($PKL_SIZE bytes)"
              cp "$PKL_FILE" actions-data-download/intraday_stock_data_${TODAY}.pkl
              cp "$PKL_FILE" actions-data-download/intraday_1m_candles.pkl
              FILES_COPIED=$((FILES_COPIED + 1))
              break
            fi
          fi
        done
        
        # Copy any database files
        cp ../pkbrokers/kite/examples/results/Data/*.db actions-data-download/ 2>/dev/null || true
        
        echo "Total files copied: $FILES_COPIED"
        
        # Commit and push
        git config user.name "github-actions[bot]"
        git config user.email "actions@github.com"
        # Use -f to force add files that may be in .gitignore
        git add -f actions-data-download/ 2>/dev/null || true
        
        if ! git diff --staged --quiet; then
          git commit -m "Update pkl files from history download - $TODAY [period: ${{ inputs.period }}]"
          git push
          echo "✅ Successfully committed updated pkl files to PKScreener actions-data-download"
        else
          echo "No changes to commit"
        fi

    - name: Send notification about the completion
      shell: bash
      env:
          THIS_KITE_PERIOD : ${{ inputs.period }}
          THIS_KITE_TOKEN: ${{ github.event.inputs.kiteToken || secrets.KTOKEN }}
      run: |
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        python3 -c "import os; from PKDevTools.classes.Telegram import send_message; send_message('✅ Successfully finished child job with valid Kite token:(' + os.environ['THIS_KITE_TOKEN'] + ') for period: '+os.environ['THIS_KITE_PERIOD']+'\n');"

  trigger-next-child-job:
    needs: [historical-data]
    if: (github.event_name == 'workflow_dispatch' || github.event_name == 'schedule')
    runs-on: ubuntu-latest
    timeout-minutes: 358
    env:
      PKDevTools_Default_Log_Level: ${{ github.event.inputs.logLevel || 20 }}
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Determine next period
      id: next-period
      run: |
        CURRENT_PERIOD="${{ github.event.inputs.period }}"
        echo "Current period: $CURRENT_PERIOD"
        
        case "$CURRENT_PERIOD" in
            "day")
                NEXT_PERIOD="minute"
                ;;
            "minute")
                NEXT_PERIOD="5minute"
                ;;
            "5minute")
                NEXT_PERIOD="10minute"
                ;;
            "10minute")
                NEXT_PERIOD="30minute"
                ;;
            "30minute")
                NEXT_PERIOD="60minute"
                ;;
            "60minute")
                echo "✅ All periods completed. Exiting."
                echo "should_exit=true" >> $GITHUB_OUTPUT
                exit 0
                ;;
            *)
                # Default to starting from day if no period specified
                NEXT_PERIOD="day"
                ;;
        esac
        
        echo "Next period: $NEXT_PERIOD"
        echo "next_period=$NEXT_PERIOD" >> $GITHUB_OUTPUT
        echo "should_exit=false" >> $GITHUB_OUTPUT

    - name: Create .env.dev file with secrets
      env:
        CHAT_ID: ${{ secrets.CHAT_ID }}
        TOKEN: ${{ secrets.TOKEN }}
        chat_idADMIN: ${{ secrets.chat_idADMIN }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        CI_PAT: ${{ secrets.CI_PAT }}
        TURSO_TOKEN: ${{ secrets.TURSO_TOKEN }}
        TDU: ${{ secrets.TDU }}
        TAT: ${{ secrets.TAT }}
        MCU: ${{ secrets.MCU }}
        MCAP: ${{ secrets.MCAP }}
        MCL: ${{ secrets.MCL }}
        MS: ${{ secrets.MS }}
        PKG: ${{ secrets.PKG }}
        REPO_URL: ${{ secrets.REPO_URL }}
        KTOKEN: ${{ inputs.kiteToken || secrets.KTOKEN }}
        KUSER: ${{ secrets.KUSER }}
        KPWD: ${{ secrets.KPWD }}
        KTOTP: ${{ secrets.KTOTP }}
        DB_TYPE: ${{ secrets.DB_TYPE }}
        TBTOKEN: ${{ secrets.TBTOKEN }}
        Tel_API_ID: ${{ secrets.Tel_API_ID }}
        Tel_API_Hash: ${{ secrets.Tel_API_Hash }}
        Tel_Phone_Number: ${{ secrets.Tel_Phone_Number }}
        TEL_SESSION_DATA: ${{ secrets.TEL_SESSION_DATA }}
        PERIOD: ${{ inputs.period }}
      run: |
        cat > .env.dev << EOF
        CHAT_ID=$CHAT_ID
        TOKEN=$TOKEN
        chat_idADMIN=$chat_idADMIN
        GITHUB_TOKEN=$GITHUB_TOKEN
        CI_PAT=$CI_PAT
        TURSO_TOKEN=$TURSO_TOKEN
        TDU=$TDU
        TAT=$TAT
        MCU=$MCU
        MCAP=$MCAP
        MCL=$MCL
        MS=$MS
        PKG=$PKG
        REPO_URL=$REPO_URL
        KTOKEN=$KTOKEN
        KUSER=$KUSER
        KPWD=$KPWD
        KTOTP=$KTOTP
        DB_TYPE=$DB_TYPE
        TBTOKEN=$TBTOKEN
        PERIOD=$PERIOD
        Tel_API_ID=$Tel_API_ID
        Tel_API_Hash=$Tel_API_Hash
        Tel_Phone_Number=$Tel_Phone_Number
        TEL_SESSION_DATA=$TEL_SESSION_DATA
        PKDevTools_Default_Log_Level=${{ env.PKDevTools_Default_Log_Level }}
        EOF
        
        echo "Created .env.dev file for period: day"

    - name: Exit if all periods completed
      if: steps.next-period.outputs.should_exit == 'true'
      run: |
        echo "✅ All periods have been processed. Workflow completed."
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        pip install pkdevtools
        python3 -c "import os; from PKDevTools.classes.Telegram import send_message; send_message('✅ All periods for historical data jobs have been processed. Workflow completed.');"
        exit 0

    - name: Trigger next child job
      if: steps.next-period.outputs.should_exit == 'false'
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        CI_PAT: ${{ secrets.CI_PAT }}
      run: |
        NEXT_PERIOD="${{ steps.next-period.outputs.next_period }}"
        echo "Triggering next child job for period: $NEXT_PERIOD"
        
        LOG_LEVEL="${{ github.event.inputs.logLevel || '20' }}"
        KITE_TOKEN="${{ github.event.inputs.kiteToken || secrets.KTOKEN }}"
        
        # Trigger child workflow for the next period only
        curl -X POST \
          -H "Authorization: token $GITHUB_TOKEN" \
          -H "Accept: application/vnd.github.v3+json" \
          "https://api.github.com/repos/${{ github.repository }}/actions/workflows/w1-workflow-history-data-child.yml/dispatches" \
          -d "{\"ref\":\"main\",\"inputs\":{\"period\":\"$NEXT_PERIOD\",\"logLevel\":\"$LOG_LEVEL\",\"kiteToken\":\"$KITE_TOKEN\",\"pastoffset\":\"${{ github.event.inputs.pastoffset || 0 }}\"}}"
        
        echo "✅ Successfully triggered next child job for period: $NEXT_PERIOD"

    - name: Send notification about the trigger
      shell: bash
      env:
          THIS_KITE_PERIOD : ${{ steps.next-period.outputs.next_period }}
          THIS_KITE_TOKEN: ${{ github.event.inputs.kiteToken || secrets.KTOKEN }}
      run: |
        pip install pkdevtools
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        python3 -c "import os; from PKDevTools.classes.Telegram import send_message; send_message('✅ Successfully triggered child job with valid Kite token:(' + os.environ['THIS_KITE_TOKEN'] + ') for period: '+os.environ['THIS_KITE_PERIOD']+'\n');"

    - name: Log completion
      run: |
        echo "Next period job triggered successfully. Workflow will continue with the next period in the chain."
