# """
#     The MIT License (MIT)

#     Copyright (c) 2023 pkjmesra

#     Permission is hereby granted, free of charge, to any person obtaining a copy
#     of this software and associated documentation files (the "Software"), to deal
#     in the Software without restriction, including without limitation the rights
#     to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
#     copies of the Software, and to permit persons to whom the Software is
#     furnished to do so, subject to the following conditions:

#     The above copyright notice and this permission notice shall be included in all
#     copies or substantial portions of the Software.

#     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
#     IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
#     FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
#     AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
#     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
#     OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
#     SOFTWARE.
# """

# =============================================================================
# WORKFLOW SUMMARY:
# This workflow (1. History Data Child) is responsible for fetching and storing
# historical market data for different time periods (day, minute, 5minute, etc.).
#
# CURRENT FUNCTIONALITY:
# 1. Calculates missing trading days by checking existing data files
# 2. Fetches historical data using pkkite.py for specified period
# 3. Commits instruments.db to GitHub (new step)
# 4. Exports database to PKL files using unified generator
# 5. Commits updated PKL files to PKScreener actions-data-download branch
# 6. Chains to next period (day -> minute) automatically
#
# UPDATED BEHAVIOR:
# - Market hours check (9:15 AM - 3:30 PM IST, non-holiday): Merges ticks.json data
# - Outside market hours or holidays: Commits PKL file as-is without merging ticks
# - PKL file size validation: Ensures daily files are >= 35MB
# - Uses cache_file_name from Archiver for date instead of current date
# - instruments.db is committed before market check to ensure token mappings are available
# =============================================================================

name: 1. History Data Child
on:
  workflow_dispatch:  # Allow manual triggering
    inputs:
      logLevel:
        description: 'Log level for PKDevTools (default 20)'
        required: false
        default: 20
        type: number
      period:
        description: 'Specific period to run (default: day)'
        required: false
        default: 'day'
        type: string
      kiteToken:
        description: 'Kite API token'
        required: false
        type: string
      pastoffset:
        description: 'Past number of days for which data has to be fetched'
        required: false
        default: 0
        type: number

jobs:
  historical-data:
    if: (github.event_name == 'workflow_dispatch' || github.event_name == 'schedule')
    runs-on: ubuntu-latest
    timeout-minutes: 358
    env:
      PKDevTools_Default_Log_Level: ${{ github.event.inputs.logLevel || 20 }}
      TEL_SESSION_DATA: ${{ secrets.TEL_SESSION_DATA }}
    steps:

    - name: Mask sensitive outputs
      # Step: Hide sensitive token values in logs
      run: |
        # Mask the token output from inputs
        echo "::add-mask::${{ inputs.kiteToken || secrets.KTOKEN }}"

    - name: Checkout PKBrokers code
      # Step: Get the PKBrokers repository code
      uses: actions/checkout@v4
      with:
        repository: pkjmesra/PKBrokers
        ref: main

    - name: Set up Python
      # Step: Configure Python 3.11 environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      # Step: Install required Python packages
      run: 
        pip install -e . kiteconnect pkdevtools libsql

    - name: Create .env.dev file with secrets
      # Step: Create environment file with all secrets for the job
      env:
        CHAT_ID: ${{ secrets.CHAT_ID }}
        TOKEN: ${{ secrets.TOKEN }}
        chat_idADMIN: ${{ secrets.chat_idADMIN }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        CI_PAT: ${{ secrets.CI_PAT }}
        TURSO_TOKEN: ${{ secrets.TURSO_TOKEN }}
        TDU: ${{ secrets.TDU }}
        TAT: ${{ secrets.TAT }}
        MCU: ${{ secrets.MCU }}
        MCAP: ${{ secrets.MCAP }}
        MCL: ${{ secrets.MCL }}
        MS: ${{ secrets.MS }}
        PKG: ${{ secrets.PKG }}
        REPO_URL: ${{ secrets.REPO_URL }}
        KTOKEN: ${{ inputs.kiteToken || secrets.KTOKEN }}
        KUSER: ${{ secrets.KUSER }}
        KPWD: ${{ secrets.KPWD }}
        KTOTP: ${{ secrets.KTOTP }}
        DB_TYPE: local
        TBTOKEN: ${{ secrets.TBTOKEN }}
        Tel_API_ID: ${{ secrets.Tel_API_ID }}
        Tel_API_Hash: ${{ secrets.Tel_API_Hash }}
        Tel_Phone_Number: ${{ secrets.Tel_Phone_Number }}
        TEL_SESSION_DATA: ${{ secrets.TEL_SESSION_DATA }}
        PERIOD: ${{ inputs.period }}
      run: |
        cat > .env.dev << EOF
        CHAT_ID=$CHAT_ID
        TOKEN=$TOKEN
        chat_idADMIN=$chat_idADMIN
        GITHUB_TOKEN=$GITHUB_TOKEN
        CI_PAT=$CI_PAT
        TURSO_TOKEN=$TURSO_TOKEN
        TDU=$TDU
        TAT=$TAT
        MCU=$MCU
        MCAP=$MCAP
        MCL=$MCL
        MS=$MS
        PKG=$PKG
        REPO_URL=$REPO_URL
        KTOKEN=$KTOKEN
        KUSER=$KUSER
        KPWD=$KPWD
        KTOTP=$KTOTP
        DB_TYPE=local
        TBTOKEN=$TBTOKEN
        PERIOD=$PERIOD
        Tel_API_ID=$Tel_API_ID
        Tel_API_Hash=$Tel_API_Hash
        Tel_Phone_Number=$Tel_Phone_Number
        TEL_SESSION_DATA=$TEL_SESSION_DATA
        PKDevTools_Default_Log_Level=${{ env.PKDevTools_Default_Log_Level }}
        EOF
        
        echo "Created .env.dev file for period: day"

    - name: Create session file
      # Step: Create Telegram session file from base64-encoded secret
      run: |
        echo ${{ env.TEL_SESSION_DATA }} | base64 -d > user_session.session
        chmod 600 user_session.session
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        echo ${{ env.TEL_SESSION_DATA }} | base64 -d > user_session.session
        chmod 600 user_session.session

    - name: Calculate missing trading days
      # Step: Determine how many days of historical data need to be fetched
      # This checks existing PKL files on GitHub and calculates the gap from latest data to today
      id: calc_offset
      if: inputs.period == 'day'
      run: |
        cd pkbrokers/kite/examples/
        mkdir -p results/Data
        
        # If pastoffset is provided and > 0, use it
        INPUT_OFFSET=${{ inputs.pastoffset || 0 }}
        
        if [ "$INPUT_OFFSET" -gt 0 ]; then
          echo "Using provided pastoffset: $INPUT_OFFSET"
          echo "PAST_OFFSET=$INPUT_OFFSET" >> $GITHUB_OUTPUT
          exit 0
        fi
        
        # Calculate missing trading days using Archiver
        echo "Calculating missing trading days..."
        
        # Install required packages if not already present
        pip install pkdevtools requests
        
        CALCULATED_OFFSET=$(python3 -c "
        import sys, pickle, requests
        from datetime import datetime, timedelta
        import os

        DEFAULT = 5
        LATEST_FILE = None
        LATEST_DATE = None

        try:
            from PKDevTools.classes.PKDateUtilities import PKDateUtilities
            from PKDevTools.classes import Archiver
            
            # Method 1: Try to get cache file name from Archiver
            print('Checking for cache file from Archiver...', file=sys.stderr)
            try:
                cache_exists, cache_file_name = Archiver.afterMarketStockDataExists()
                if cache_exists and cache_file_name:
                    url = f'https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/actions-data-download/{cache_file_name}'
                    print(f'Trying Archiver cache file: {cache_file_name}', file=sys.stderr)
                    try:
                        r = requests.get(url, timeout=60)
                        if r.status_code == 200 and len(r.content) > 1000000:
                            data = pickle.loads(r.content)
                            if len(data) > 100:
                                print(f'âœ… Found Archiver cache file: {cache_file_name}', file=sys.stderr)
                                LATEST_FILE = url
                                LATEST_DATE = cache_file_name.replace('stock_data_', '').replace('.pkl', '')
                                try:
                                    LATEST_DATE = datetime.strptime(LATEST_DATE, '%d%m%Y').date()
                                except:
                                    LATEST_DATE = None
                    except Exception as e:
                        print(f'Error loading Archiver cache file: {e}', file=sys.stderr)
            except Exception as e:
                print(f'Archiver method failed: {e}', file=sys.stderr)
            
            # Method 2: If Archiver method failed, find the latest file in the directory
            if LATEST_FILE is None:
                print('Falling back to finding latest file in directory...', file=sys.stderr)
                
                # Base URL for the directory listing (using GitHub API to list files)
                api_url = 'https://api.github.com/repos/pkjmesra/PKScreener/contents/actions-data-download?ref=actions-data-download'
                
                try:
                    headers = {}
                    # Add token if available for higher rate limits
                    if 'GITHUB_TOKEN' in os.environ:
                        headers['Authorization'] = f'token {os.environ["GITHUB_TOKEN"]}'
                    
                    r = requests.get(api_url, headers=headers, timeout=30)
                    if r.status_code == 200:
                        files = r.json()
                        pkl_files = []
                        
                        for file in files:
                            if file['name'].startswith('stock_data_') and file['name'].endswith('.pkl'):
                                # Extract date from filename (stock_data_DDMMYYYY.pkl)
                                date_str = file['name'].replace('stock_data_', '').replace('.pkl', '')
                                try:
                                    file_date = datetime.strptime(date_str, '%d%m%Y').date()
                                    pkl_files.append((file_date, file['name'], file['download_url']))
                                except:
                                    continue
                        
                        if pkl_files:
                            # Sort by date (latest first)
                            pkl_files.sort(key=lambda x: x[0], reverse=True)
                            latest_date, latest_name, latest_url = pkl_files[0]
                            print(f'Latest file found: {latest_name} from {latest_date}', file=sys.stderr)
                            
                            # Download the latest file
                            r = requests.get(latest_url, timeout=60)
                            if r.status_code == 200 and len(r.content) > 1000000:
                                data = pickle.loads(r.content)
                                if len(data) > 100:
                                    LATEST_FILE = latest_url
                                    LATEST_DATE = latest_date
                except Exception as e:
                    print(f'Error listing directory: {e}', file=sys.stderr)
            
            # Method 3: Fallback to hardcoded URLs if both methods fail
            if LATEST_FILE is None:
                print('Falling back to hardcoded URLs...', file=sys.stderr)
                urls = [
                    'https://raw.githubusercontent.com/pkjmesra/PKScreener/actions-data-download/actions-data-download/stock_data_17122025.pkl',
                ]
                
                for url in urls:
                    try:
                        r = requests.get(url, timeout=60)
                        if r.status_code == 200 and len(r.content) > 1000000:
                            data = pickle.loads(r.content)
                            if len(data) > 100:
                                print(f'Found pkl: {url}', file=sys.stderr)
                                LATEST_FILE = url
                                # Extract date from URL
                                date_str = url.split('stock_data_')[-1].split('.pkl')[0]
                                try:
                                    LATEST_DATE = datetime.strptime(date_str, '%d%m%Y').date()
                                except:
                                    LATEST_DATE = None
                                break
                    except: 
                        continue
            
            # If no data found, return default offset
            if LATEST_FILE is None:
                print('No data found, using default offset', file=sys.stderr)
                print(DEFAULT)
                sys.exit(0)
            
            # Now we have data, find the latest date in it
            if LATEST_DATE is None:
                # If we couldn't get date from filename, extract from data
                latest_data_date = None
                for symbol, df in list(data.items())[:50]:
                    try:
                        if hasattr(df, 'index') and len(df) > 0:
                            max_date = df.index.max()
                            if hasattr(max_date, 'date'):
                                max_date = max_date.date()
                            if latest_data_date is None or max_date > latest_data_date:
                                latest_data_date = max_date
                    except: 
                        continue
                
                if latest_data_date:
                    LATEST_DATE = latest_data_date
                else:
                    print('Could not determine latest date from data', file=sys.stderr)
                    print(DEFAULT)
                    sys.exit(0)
            
            print(f'Latest date in data: {LATEST_DATE}', file=sys.stderr)
            
            # Get today's trading date
            lt = PKDateUtilities.tradingDate()
            if hasattr(lt, 'date'):
                lt = lt.date()
            print(f'Last trading date: {lt}', file=sys.stderr)
            
            # Compare dates
            if LATEST_DATE >= lt:
                print(0)
                sys.exit(0)
            
            # Calculate missing trading days
            try:
                missing = PKDateUtilities.trading_days_between(LATEST_DATE, lt)
                print(f'Missing trading days: {missing}', file=sys.stderr)
                print(max(1, missing))
            except:
                # Fallback calculation
                diff = (lt - LATEST_DATE).days
                print(max(1, int(diff * 5 / 7)))
                
        except Exception as e:
            print(f'Error: {e}', file=sys.stderr)
            print(DEFAULT)
        " 2>&1 | tee /dev/stderr | tail -1)
        
        # Ensure we have a valid number
        if [ -z "$CALCULATED_OFFSET" ] || ! [[ "$CALCULATED_OFFSET" =~ ^[0-9]+$ ]]; then
          echo "Invalid offset '$CALCULATED_OFFSET', using default 5"
          CALCULATED_OFFSET=5
        fi
        
        echo "Calculated pastoffset: $CALCULATED_OFFSET"
        echo "PAST_OFFSET=$CALCULATED_OFFSET" >> $GITHUB_OUTPUT

    - name: Run historical data for ${{ inputs.period }}
      # Step: Execute pkkite.py to fetch historical data for the specified period
      # Uses the calculated offset to determine how many days to fetch
      run: |
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        
        # Use calculated offset from previous step if available
        OFFSET=${{ steps.calc_offset.outputs.PAST_OFFSET || inputs.pastoffset || 0 }}
        echo "Running pkkite with --history=${{ inputs.period }} --pastoffset=$OFFSET"
        
        python pkkite.py --history=${{ inputs.period }} --pastoffset=$OFFSET --verbose

    - name: Find and commit instrument_history.db to PKScreener actions-data-download
      # UPDATED STEP: Commit instrument_history.db with date suffix and clean up old instrument_history_*.db files
      # This file is critical for token-to-symbol mapping in subsequent steps
      # Renames to instrument_history_YYYYMMDD.db format and removes older instrument_history_*.db files
      env:
        CI_PAT: ${{ secrets.CI_PAT }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        cd pkbrokers/kite/examples/
        
        # First, get the target date using the same logic as the pkl commit step
        echo "Getting target date from Archiver..."
        TARGET_INFO=$(python3 -c "
        from PKDevTools.classes import Archiver
        cache_exists, cache_file_name = Archiver.afterMarketStockDataExists()
        if cache_exists and cache_file_name:
            print(f'{cache_file_name}')
        else:
            print('stock_data_$(date +%d%m%Y).pkl')
        ")
        
        # Extract date from cache file name (stock_data_DDMMYYYY.pkl)
        if [[ $TARGET_INFO =~ stock_data_([0-9]{8})\.pkl ]]; then
            TARGET_DATE="${BASH_REMATCH[1]}"
        else
            TARGET_DATE=$(date +%d%m%Y)
        fi
        
        echo "Target date for instrument_history.db: $TARGET_DATE"
        
        # Find instrument_history.db in common locations
        echo "Searching for instrument_history.db..."
        
        INSTRUMENTS_DB_PATH=""
        
        # Check common locations
        POSSIBLE_PATHS=(
          "instrument_history.db"
          "results/Data/instrument_history.db"
          "../../../instrument_history.db"
          "$HOME/.PKDevTools_userdata/instrument_history.db"
          "$HOME/.pkbrokers/instrument_history.db"
        )
        
        for path in "${POSSIBLE_PATHS[@]}"; do
          if [ -f "$path" ]; then
            INSTRUMENTS_DB_PATH="$path"
            DB_SIZE=$(stat -c%s "$path" 2>/dev/null || stat -f%z "$path")
            echo "âœ… Found instrument_history.db at: $path ($DB_SIZE bytes)"
            break
          fi
        done
        
        if [ -z "$INSTRUMENTS_DB_PATH" ]; then
          echo "âš ï¸ No instrument_history.db found, skipping commit"
          exit 0
        fi
        
        # Clone PKScreener actions-data-download branch
        git clone --single-branch --branch actions-data-download https://x-access-token:$CI_PAT@github.com/pkjmesra/PKScreener.git pkscreener-data
        cd pkscreener-data
        
        # Create dated filename
        DATED_INSTRUMENTS_FILE="instrument_history_${{ inputs.period }}_${TARGET_DATE}.db"
        echo "Dated instrument_history file: $DATED_INSTRUMENTS_FILE"
        
        # Copy instrument_history.db to the repository with dated name
        mkdir -p actions-data-download
        cp "../$INSTRUMENTS_DB_PATH" "actions-data-download/$DATED_INSTRUMENTS_FILE"
        
        # Delete any older instrument_history_*.db files (except the current one)
        echo "Removing older instrument_history_*.db files..."
        OLD_FILES=$(ls actions-data-download/instrument_history_${{ inputs.period }}_*.db 2>/dev/null | grep -v "$DATED_INSTRUMENTS_FILE" || true)
        if [ -n "$OLD_FILES" ]; then
          echo "Found old instrument_history files to remove:"
          echo "$OLD_FILES"
          rm -f $OLD_FILES
        else
          echo "No old instrument_history files found"
        fi
        
        # Configure git
        git config user.name "github-actions[bot]"
        git config user.email "actions@github.com"
        
        # Add all changes (new file and deletions)
        git add -A actions-data-download/
        
        # Show what's changed
        echo "Changes to be committed:"
        git diff --staged --name-status
        
        if ! git diff --staged --quiet; then
          git commit -m "Update instrument_history.db to $DATED_INSTRUMENTS_FILE and cleanup old versions [skip ci]"
          
          # Retry push with rebase if needed
          for i in 1 2 3; do
            if git push; then
              echo "âœ… Successfully committed $DATED_INSTRUMENTS_FILE to PKScreener actions-data-download"
              break
            else
              echo "Push failed, attempt $i, pulling and retrying..."
              git pull --rebase origin actions-data-download || true
              sleep 2
            fi
          done
        else
          echo "No changes to instrument_history.db"
        fi
        
        cd ../..

    - name: Check if market is open for tick merging
      # Step: Determine if we should merge ticks.json data based on market hours and holidays
      # Market hours: 9:15 AM to 3:30 PM IST on non-holiday weekdays
      # Outside these hours or on holidays: Skip merging and commit PKL as-is
      id: market_check
      if: inputs.period == 'day'
      run: |
        cd pkbrokers/kite/examples/
        
        # Install pkdevtools if not already present
        pip install pkdevtools
        
        SHOULD_MERGE_TICKS=$(python3 -c "
        from PKDevTools.classes.PKDateUtilities import PKDateUtilities
        from datetime import datetime, time
        import pytz
        import sys
        
        # Check if today is a holiday
        is_holiday = PKDateUtilities.isTodayHoliday()
        if is_holiday:
            print('false')
            sys.exit(0)
        
        # Get current time in IST
        ist = pytz.timezone('Asia/Kolkata')
        now = datetime.now(ist).time()
        
        # Market hours: 9:15 AM to 3:30 PM IST
        market_start = time(9, 15)
        market_end = time(15, 30)
        
        if market_start <= now <= market_end:
            print('true')
        else:
            print('false')
        ")
        
        echo "Should merge ticks.json data: $SHOULD_MERGE_TICKS"
        echo "MERGE_TICKS=$SHOULD_MERGE_TICKS" >> $GITHUB_OUTPUT

    - name: Export database to pkl files using unified generator
      # Step: Convert SQLite database to PKL format for efficient storage
      # For day period only
      # If market is open and not holiday, merges ticks.json data with database
      # If market is closed or holiday, exports database as-is without merging
      if: inputs.period == 'day'
      run: |
        cd pkbrokers/kite/examples/
        mkdir -p results/Data
        
        MERGE_FLAG="${{ steps.market_check.outputs.MERGE_TICKS }}"
        
        if [ "$MERGE_FLAG" == "true" ]; then
            echo "ðŸŸ¢ Market is open - merging ticks.json data with database..."
            echo "Using unified pkl generator script (with tick merging)..."
            
            # Without --from-db flag, it will load from ticks.json and merge with historical data
            python3 ../../../pkbrokers/scripts/generate_pkl_from_ticks.py \
              --data-dir results/Data \
              --verbose \
              --past-offset ${{ steps.calc_offset.outputs.PAST_OFFSET || inputs.pastoffset || 0 }}
        else
            echo "ðŸ”´ Market is closed OR today is a holiday - exporting database as-is (no tick merging)..."
            echo "Using unified pkl generator script (--from-db mode only)..."
            
            # With --from-db flag, it will load from SQLite database only
            python3 ../../../pkbrokers/scripts/generate_pkl_from_ticks.py \
              --from-db \
              --data-dir results/Data \
              --verbose \
              --past-offset ${{ steps.calc_offset.outputs.PAST_OFFSET || inputs.pastoffset || 0 }}
        fi
        
        echo ""
        echo "Generated pkl files:"
        ls -la results/Data/*.pkl 2>/dev/null || echo "No pkl files found"

    - name: Commit updated pkl file to PKScreener actions-data-download
      # Step: Upload the generated PKL files to the PKScreener repository
      # Uses cache_file_name from Archiver for date instead of current date
      # Validates daily PKL files are at least 25MB before committing
      env:
        CI_PAT: ${{ secrets.CI_PAT }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Get cache file name from Archiver to determine the correct date
        CACHE_INFO=$(python3 -c "
        from PKDevTools.classes import Archiver
        cache_exists, cache_file_name = Archiver.afterMarketStockDataExists()
        if cache_exists and cache_file_name:
            print(f'{cache_file_name}')
        else:
            print('stock_data_$(date +%d%m%Y).pkl')
        ")
        
        # Extract date from cache file name (stock_data_DDMMYYYY.pkl)
        if [[ $CACHE_INFO =~ stock_data_([0-9]{8})\.pkl ]]; then
            TARGET_DATE="${BASH_REMATCH[1]}"
        else
            TARGET_DATE=$(date +%d%m%Y)
        fi
        
        echo "Using target date from cache: $TARGET_DATE"
        
        # Clone PKScreener actions-data-download branch
        git clone --single-branch --branch actions-data-download https://x-access-token:$CI_PAT@github.com/pkjmesra/PKScreener.git pkscreener-data
        cd pkscreener-data
        
        PERIOD="${{ inputs.period }}"
        
        echo "Period: $PERIOD"
        echo "Target date: $TARGET_DATE"
        
        # Determine pkl filename based on period
        # day -> stock_data_DDMMYYYY.pkl
        # minute (1-min) -> intraday_stock_data_DDMMYYYY.pkl
        # 5minute -> intraday_stock_data_DDMMYYYY_5min.pkl
        # 10minute -> intraday_stock_data_DDMMYYYY_10min.pkl
        # 30minute -> intraday_stock_data_DDMMYYYY_30min.pkl
        # 60minute -> intraday_stock_data_DDMMYYYY_60min.pkl
        
        case "$PERIOD" in
            "day")
                TARGET_FILE="stock_data_${TARGET_DATE}.pkl"
                GENERIC_FILE="daily_candles.pkl"
                MIN_SIZE=25000000  # 25 MB minimum for daily (fixed from 1MB to 25MB)
                ;;
            "minute")
                TARGET_FILE="intraday_stock_data_${TARGET_DATE}.pkl"
                GENERIC_FILE="intraday_1m_candles.pkl"
                MIN_SIZE=10000  # 10 KB minimum for intraday
                ;;
            "5minute")
                TARGET_FILE="intraday_stock_data_${TARGET_DATE}_5min.pkl"
                GENERIC_FILE="intraday_5m_candles.pkl"
                MIN_SIZE=10000
                ;;
            "10minute")
                TARGET_FILE="intraday_stock_data_${TARGET_DATE}_10min.pkl"
                GENERIC_FILE="intraday_10m_candles.pkl"
                MIN_SIZE=10000
                ;;
            "30minute")
                TARGET_FILE="intraday_stock_data_${TARGET_DATE}_30min.pkl"
                GENERIC_FILE="intraday_30m_candles.pkl"
                MIN_SIZE=10000
                ;;
            "60minute")
                TARGET_FILE="intraday_stock_data_${TARGET_DATE}_60min.pkl"
                GENERIC_FILE="intraday_60m_candles.pkl"
                MIN_SIZE=10000
                ;;
            *)
                echo "Unknown period: $PERIOD, skipping pkl commit"
                exit 0
                ;;
        esac
        
        echo "Target file: $TARGET_FILE"
        echo "Generic file: $GENERIC_FILE"
        echo "Minimum size required: $MIN_SIZE bytes"
        
        # List available files
        echo "Files in ../pkbrokers/kite/examples/results/Data/:"
        ls -la ../pkbrokers/kite/examples/results/Data/ 2>/dev/null || echo "Directory not found"
        
        FILES_COPIED=0
        
        if [ "$PERIOD" = "day" ]; then
            # Daily pkl files
            for PKL_FILE in ../pkbrokers/kite/examples/results/Data/stock_data_*.pkl ../pkbrokers/kite/examples/results/Data/daily_candles.pkl; do
              if [ -f "$PKL_FILE" ]; then
                PKL_SIZE=$(stat -c%s "$PKL_FILE" 2>/dev/null || stat -f%z "$PKL_FILE")
                if [ "$PKL_SIZE" -gt "$MIN_SIZE" ]; then
                  echo "âœ… Found valid daily pkl: $PKL_FILE ($PKL_SIZE bytes)"
                  cp "$PKL_FILE" actions-data-download/$TARGET_FILE
                  cp "$PKL_FILE" actions-data-download/$GENERIC_FILE
                  FILES_COPIED=$((FILES_COPIED + 1))
                  break
                else
                  echo "âš ï¸ File too small (needs >25MB): $PKL_FILE ($PKL_SIZE bytes)"
                fi
              fi
            done
            
            # Also try to commit intraday from ticks.json for day period
            INTRADAY_FOUND=0
            for PKL_FILE in ../pkbrokers/kite/examples/results/Data/intraday_stock_data_*.pkl ../pkbrokers/kite/examples/results/Data/intraday_1m_candles.pkl; do
              if [ -f "$PKL_FILE" ]; then
                PKL_SIZE=$(stat -c%s "$PKL_FILE" 2>/dev/null || stat -f%z "$PKL_FILE")
                if [ "$PKL_SIZE" -gt 10000 ]; then
                  echo "âœ… Found intraday: $PKL_FILE ($PKL_SIZE bytes)"
                  cp "$PKL_FILE" actions-data-download/intraday_stock_data_${TARGET_DATE}.pkl
                  cp "$PKL_FILE" actions-data-download/intraday_1m_candles.pkl
                  FILES_COPIED=$((FILES_COPIED + 1))
                  INTRADAY_FOUND=1
                  break
                fi
              fi
            done
            
            # If no intraday pkl, try to generate from ticks.json
            if [ "$INTRADAY_FOUND" -eq 0 ]; then
              echo "No intraday pkl found, trying to generate from ticks.json..."
              cd ../pkbrokers/kite/examples/
              python3 ../../../pkbrokers/scripts/generate_pkl_from_ticks.py --data-dir results/Data --verbose 2>/dev/null || true
              cd -
              
              for PKL_FILE in ../pkbrokers/kite/examples/results/Data/intraday_stock_data_*.pkl; do
                if [ -f "$PKL_FILE" ]; then
                  PKL_SIZE=$(stat -c%s "$PKL_FILE" 2>/dev/null || stat -f%z "$PKL_FILE")
                  if [ "$PKL_SIZE" -gt 10000 ]; then
                    echo "âœ… Generated intraday: $PKL_FILE ($PKL_SIZE bytes)"
                    cp "$PKL_FILE" actions-data-download/intraday_stock_data_${TARGET_DATE}.pkl
                    cp "$PKL_FILE" actions-data-download/intraday_1m_candles.pkl
                    FILES_COPIED=$((FILES_COPIED + 1))
                    break
                  fi
                fi
              done
            fi
        else
            # Intraday periods (minute, 5minute, 10minute, etc.)
            # Look for the history database file generated by pkkite
            for PKL_FILE in ../pkbrokers/kite/examples/results/Data/*.pkl; do
              if [ -f "$PKL_FILE" ]; then
                PKL_SIZE=$(stat -c%s "$PKL_FILE" 2>/dev/null || stat -f%z "$PKL_FILE")
                if [ "$PKL_SIZE" -gt "$MIN_SIZE" ]; then
                  echo "âœ… Found: $PKL_FILE ($PKL_SIZE bytes)"
                  cp "$PKL_FILE" actions-data-download/$TARGET_FILE
                  cp "$PKL_FILE" actions-data-download/$GENERIC_FILE
                  FILES_COPIED=$((FILES_COPIED + 1))
                  break
                fi
              fi
            done
        fi
        
        # DO NOT copy .db files - they are too large for GitHub (>100MB limit)
        # Note: instruments.db is committed separately in an earlier step
        echo "Note: SQLite .db files are not committed here (instruments.db committed separately)"
        
        echo "Total files copied: $FILES_COPIED"
        
        if [ "$FILES_COPIED" -eq 0 ]; then
          echo "âš ï¸ No valid pkl files found to commit for period: $PERIOD"
          exit 0
        fi
        
        # Commit and push
        git config user.name "github-actions[bot]"
        git config user.email "actions@github.com"
        
        # Remove any .db files that may have been accidentally copied (too large for GitHub)
        rm -f actions-data-download/*.db 2>/dev/null || true
        
        # Only add pkl and json files (no .db files - they exceed GitHub's 100MB limit)
        git add -f actions-data-download/*.pkl 2>/dev/null || true
        git add -f actions-data-download/*.json 2>/dev/null || true
        
        # Show what's staged
        echo "Files to be committed:"
        git diff --staged --name-only
        
        if ! git diff --staged --quiet; then
          git commit -m "Update $TARGET_FILE from history download - $TARGET_DATE [period: $PERIOD]"
          
          # Retry push with rebase if needed
          for i in 1 2 3; do
            if git push; then
              echo "âœ… Successfully committed updated pkl files to PKScreener actions-data-download"
              break
            else
              echo "Push failed, attempt $i, pulling and retrying..."
              git pull --rebase origin actions-data-download || true
              sleep 2
            fi
          done
        else
          echo "No changes to commit"
        fi

    - name: Send notification about the completion
      # Step: Send Telegram notification about successful completion
      shell: bash
      env:
          THIS_KITE_PERIOD : ${{ inputs.period }}
          THIS_KITE_TOKEN: ${{ github.event.inputs.kiteToken || secrets.KTOKEN }}
      run: |
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        python3 -c "import os; from PKDevTools.classes.Telegram import send_message; send_message('âœ… Successfully finished child job with valid Kite token:(' + os.environ['THIS_KITE_TOKEN'] + ') for period: '+os.environ['THIS_KITE_PERIOD']+'\n');"

  trigger-next-child-job:
    # Job: Chain to the next time period (day -> minute)
    needs: [historical-data]
    if: (github.event_name == 'workflow_dispatch' || github.event_name == 'schedule')
    runs-on: ubuntu-latest
    timeout-minutes: 358
    env:
      PKDevTools_Default_Log_Level: ${{ github.event.inputs.logLevel || 20 }}
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Determine next period
      # Step: Decide which period to run next (day -> minute, then stop)
      id: next-period
      run: |
        CURRENT_PERIOD="${{ github.event.inputs.period }}"
        echo "Current period: $CURRENT_PERIOD"
        
        # Only run for two intervals: day and minute (1-min)
        # Other intervals (5minute, 10minute, 30minute, 60minute) are skipped
        case "$CURRENT_PERIOD" in
            "day")
                NEXT_PERIOD="minute"
                ;;
            "minute")
                # Stop after minute - don't cascade to other intervals
                echo "âœ… All periods completed (day and minute). Exiting."
                echo "should_exit=true" >> $GITHUB_OUTPUT
                exit 0
                ;;
            *)
                # Default to starting from day if no period specified
                NEXT_PERIOD="day"
                ;;
        esac
        
        echo "Next period: $NEXT_PERIOD"
        echo "next_period=$NEXT_PERIOD" >> $GITHUB_OUTPUT
        echo "should_exit=false" >> $GITHUB_OUTPUT

    - name: Create .env.dev file with secrets
      # Step: Create environment file for the next job
      env:
        CHAT_ID: ${{ secrets.CHAT_ID }}
        TOKEN: ${{ secrets.TOKEN }}
        chat_idADMIN: ${{ secrets.chat_idADMIN }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        CI_PAT: ${{ secrets.CI_PAT }}
        TURSO_TOKEN: ${{ secrets.TURSO_TOKEN }}
        TDU: ${{ secrets.TDU }}
        TAT: ${{ secrets.TAT }}
        MCU: ${{ secrets.MCU }}
        MCAP: ${{ secrets.MCAP }}
        MCL: ${{ secrets.MCL }}
        MS: ${{ secrets.MS }}
        PKG: ${{ secrets.PKG }}
        REPO_URL: ${{ secrets.REPO_URL }}
        KTOKEN: ${{ inputs.kiteToken || secrets.KTOKEN }}
        KUSER: ${{ secrets.KUSER }}
        KPWD: ${{ secrets.KPWD }}
        KTOTP: ${{ secrets.KTOTP }}
        DB_TYPE: local
        TBTOKEN: ${{ secrets.TBTOKEN }}
        Tel_API_ID: ${{ secrets.Tel_API_ID }}
        Tel_API_Hash: ${{ secrets.Tel_API_Hash }}
        Tel_Phone_Number: ${{ secrets.Tel_Phone_Number }}
        TEL_SESSION_DATA: ${{ secrets.TEL_SESSION_DATA }}
        PERIOD: ${{ inputs.period }}
      run: |
        cat > .env.dev << EOF
        CHAT_ID=$CHAT_ID
        TOKEN=$TOKEN
        chat_idADMIN=$chat_idADMIN
        GITHUB_TOKEN=$GITHUB_TOKEN
        CI_PAT=$CI_PAT
        TURSO_TOKEN=$TURSO_TOKEN
        TDU=$TDU
        TAT=$TAT
        MCU=$MCU
        MCAP=$MCAP
        MCL=$MCL
        MS=$MS
        PKG=$PKG
        REPO_URL=$REPO_URL
        KTOKEN=$KTOKEN
        KUSER=$KUSER
        KPWD=$KPWD
        KTOTP=$KTOTP
        DB_TYPE=local
        TBTOKEN=$TBTOKEN
        PERIOD=$PERIOD
        Tel_API_ID=$Tel_API_ID
        Tel_API_Hash=$Tel_API_Hash
        Tel_Phone_Number=$Tel_Phone_Number
        TEL_SESSION_DATA=$TEL_SESSION_DATA
        PKDevTools_Default_Log_Level=${{ env.PKDevTools_Default_Log_Level }}
        EOF
        
        echo "Created .env.dev file for period: day"

    - name: Exit if all periods completed
      # Step: Exit workflow if all periods are processed
      if: steps.next-period.outputs.should_exit == 'true'
      run: |
        echo "âœ… All periods have been processed. Workflow completed."
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        pip install pkdevtools
        python3 -c "import os; from PKDevTools.classes.Telegram import send_message; send_message('âœ… All periods for historical data jobs have been processed. Workflow completed.');"
        exit 0

    - name: Trigger next child job
      # Step: Dispatch the workflow for the next period
      if: steps.next-period.outputs.should_exit == 'false'
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        CI_PAT: ${{ secrets.CI_PAT }}
      run: |
        NEXT_PERIOD="${{ steps.next-period.outputs.next_period }}"
        echo "Triggering next child job for period: $NEXT_PERIOD"
        
        LOG_LEVEL="${{ github.event.inputs.logLevel || '20' }}"
        KITE_TOKEN="${{ github.event.inputs.kiteToken || secrets.KTOKEN }}"
        
        # Trigger child workflow for the next period only
        curl -X POST \
          -H "Authorization: token $GITHUB_TOKEN" \
          -H "Accept: application/vnd.github.v3+json" \
          "https://api.github.com/repos/${{ github.repository }}/actions/workflows/w1-workflow-history-data-child.yml/dispatches" \
          -d "{\"ref\":\"main\",\"inputs\":{\"period\":\"$NEXT_PERIOD\",\"logLevel\":\"$LOG_LEVEL\",\"kiteToken\":\"$KITE_TOKEN\",\"pastoffset\":\"${{ github.event.inputs.pastoffset || 0 }}\"}}"
        
        echo "âœ… Successfully triggered next child job for period: $NEXT_PERIOD"

    - name: Send notification about the trigger
      # Step: Send Telegram notification about triggering next job
      shell: bash
      env:
          THIS_KITE_PERIOD : ${{ steps.next-period.outputs.next_period }}
          THIS_KITE_TOKEN: ${{ github.event.inputs.kiteToken || secrets.KTOKEN }}
      run: |
        pip install pkdevtools
        cp .env.dev pkbrokers/kite/examples/.env.dev
        cd pkbrokers/kite/examples/
        python3 -c "import os; from PKDevTools.classes.Telegram import send_message; send_message('âœ… Successfully triggered child job with valid Kite token:(' + os.environ['THIS_KITE_TOKEN'] + ') for period: '+os.environ['THIS_KITE_PERIOD']+'\n');"

    - name: Log completion
      # Step: Log that next job was triggered
      run: |
        echo "Next period job triggered successfully. Workflow will continue with the next period in the chain."